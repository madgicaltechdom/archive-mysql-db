{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a8e0b0-a26f-4cdf-8feb-8ccec8e25942",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Keshav.gupta\\AppData\\Local\\Microsoft\\WindowsApps\\python3.11.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Keshav.gupta/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Replace these placeholders with your Azure Blob Storage details\n",
    "storage_account_name = \"keshavtesting\"\n",
    "container_name = \"keshav-sql\"\n",
    "access_key = \"H8woyne3OP34rUKEZ1DIfsZwMql+TSA+3ZIjGZcl9P5QVdhyfqvMpaG4DT0evI7rAtNH6ryA6AWD+AStHyJTpQ==\"\n",
    "blob_name = 'azdata.parquet'\n",
    "\n",
    "# dbutils.fs.mount(source=f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "#                  mount_point=\"/mnt/rezoarchives\",\n",
    "#                  extra_configs={f\"{storage_account_name}.blob.core.windows.net\": storage_account_key})\n",
    "# Mount the Azure Blob Storage to a Databricks filesystem (DBFS)\n",
    "\n",
    "storage_account_name = \"rezoarchives\"\n",
    "storage_account_key = \"5NkHC+EM+ouEq1t8lfpYr4MTyEW7u6YgMWNcOSmzxM6/0NnuLVSsDb23Pmb+KAj02bcjaq0qCEJv+AStszOlrw==\"\n",
    "\n",
    "# spark.conf.set(\"fs.azure.account.key.<storage acount name>.dfs.core.windows.net\",\"<storage account key>\")\n",
    "# spark.conf.set(\"fs.azure.account.key.rezoarchives.dfs.core.windows.net\",\"5NkHC+EM+ouEq1t8lfpYr4MTyEW7u6YgMWNcOSmzxM6/0NnuLVSsDb23Pmb+KAj02bcjaq0qCEJv+AStszOlrw==\")\n",
    "\n",
    "spark.conf.set('fs.azure.account.key.' + storage_account_name + '.blob.core.windows.net', access_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780e9bbe-31e7-4b15-9e46-0520e1d9fad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a Spark session\n",
    "# spark = SparkSession.builder.appName(\"ParquetReader\").getOrCreate()\n",
    "filePath = \"wasbs://\" + container_name + \"@\" + storage_account_name + \".blob.core.windows.net/\"+ blob_name\"\n",
    "df = spark.read.format(\"parquet\").load(filePath, inferSchema = True, header = True)\n",
    "# Read Parquet data from the mounted location\n",
    "# parquet_data = spark.read.parquet(\"m202307.parquet\")\n",
    "\n",
    "# Show the top 10 rows of the Parquet data\n",
    "df.show(10)\n",
    "\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ddf067-23aa-4b70-83d4-03cabdc122bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"m202307\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e153ac9-e471-498a-bcc6-c8a41dfc5334",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query_result = spark.sql(\"SELECT * FROM m202307 WHERE model_name = 'model_9'\")\n",
    "query_result.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Rezo Retrieve Parquet Format",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
